import itertools
import os
import collections
import json
from snakemake.utils import R
from snakemake.exceptions import MissingInputException
# Snakemake Base location
try:
	NGS_PIPELINE=os.environ['NGS_PIPELINE']
	WORK_DIR=os.environ['WORK_DIR']
	DATA_DIR=os.environ['DATA_DIR']
	ACT_DIR=os.environ['ACT_DIR']
except KeyError:
	NGS_PIPELINE="/data/khanlab/projects/patidar/Snakemake"
	ACT_DIR="/Actionable/"
	pass
shell.prefix("set -eo pipefail; ")
configfile: NGS_PIPELINE +"/config_common.json"
configfile: NGS_PIPELINE +"/config_cluster.json"
###########################################################################
#
#			Conversion
#
###########################################################################
SUBJECT_TO_SAMPLE  = {}
for subject in config['subject']:
	SUBJECT_TO_SAMPLE[subject] = expand("{sample}", sample = config['subject'][subject])
###########################################################################
SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['subject'].items():
	for sample in samples:
		SAMPLE_TO_SUBJECT[sample]=subject
###########################################################################
####
#### Targets
####
PATIENTS =[]
SUBS  = []
SUB_BAMS= {}
SUB_COV = {}
SUB_LOH = {}
SUB_MPG = {}
SUB_HOT = {}
SAMPLES =[]
somaticPairs = {}
somaticCopy = {}
pairedCapture = {}
QC=[]
for subject in config['subject'].keys():
	QC.extend(subject+"/qc/"+subject+".coveragePlot.png")
	QC.extend(subject+"/qc/"+subject+".circos.png")
	QC.extend(subject+"/qc/"+subject+".hotspot_coverage.png")
	QC.extend(subject+"/annotation/"+subject+".Annotations.coding.rare.txt")
	QC.extend(subject+"/igv/session_"+subject+".xml")
	SUBS.append(subject)
	PATIENTS.append(subject)
	SUB_BAMS[subject]= ["{subject}/{sample}/{sample}.bwa.final.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
	SUB_COV[subject] = ["{subject}/{sample}/qc/{sample}.bwa.coverage.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
	SUB_HOT[subject] = ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
	SUB_LOH[subject] = ["{subject}/{sample}/qc/{sample}.bwa.loh".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
	SUB_MPG[subject] = ["{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in config['subject'][subject]]
	for sample in config['subject'][subject]:
		SAMPLES.append(sample)

###########################################################################
#		Add RNASeq only samples to PATIENTS
###########################################################################
for subject  in config['RNASeq'].keys():
        if subject not in PATIENTS:
                PATIENTS.append(subject)	

###########################################################################
ALL_FASTQC  = ["{subject}/{sample}/qc/fastqc/{sample}_R2_fastqc.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_BAMS    = ["{subject}/{sample}/{sample}.bwa.bam".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_QC      = ["{subject}/{sample}/qc/{sample}.bwa.flagstat.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.hotspot.depth".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
ALL_QC     += ["{subject}/{sample}/qc/{sample}.bwa.gt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES] 
ALL_QC     += ["{subject}/{sample}/qc/BamQC/qualimapReport.html".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
ALL_QC     += ["{subject}/{sample}/copyNumber/{sample}.count.tmp.txt".format(subject=SAMPLE_TO_SUBJECT[s], sample=s) for s in SAMPLES]
QC.extend(ALL_QC)
QC.extend(ALL_FASTQC)
if len(config['sample_references']) > 0:
	for Tumor in config['sample_references']:
		for Normal in config['sample_references'][Tumor]:
			TumorBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
			TumorCopy = "{subject}/{sample}/copyNumber/{sample}.count.tmp.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)
			NormalBam = "{subject}/{sample}/{sample}.bwa.final".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
			NormalCopy = "{subject}/{sample}/copyNumber/{sample}.count.tmp.txt".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal)
			pairedCapture[Tumor] = config['sample_captures'][Tumor]
			somaticPairs[Tumor] = [TumorBam + ".bam" , TumorBam + ".bam.bai", NormalBam + ".bam", NormalBam + ".bam.bai"]
			somaticCopy[Tumor] = [NormalCopy, TumorCopy]
###########################################################################
SUBJECT_ANNO = dict([(key, {}) for key in PATIENTS])
def add_to_SUBJECT_ANNO(subject, category, file_list):
	if category not in SUBJECT_ANNO[subject]:
		SUBJECT_ANNO[subject][category] = file_list
	else:
		SUBJECT_ANNO[subject][category].extend(file_list)
###########################################################################
SUBJECT_VCFS = {}
COPY_NUMBER=[]
SOMATIC =[]
for subject in SUBS:
	local  = []
	local.extend([(subject+"/"+subject+"/calls/"+subject+".hapcaller.snpEff.txt"),
		      (subject+"/"+subject+"/calls/"+subject+".platypus.snpEff.txt"),
		      (subject+"/"+subject+"/calls/"+subject+".bam2mpg.snpEff.txt")])
	if subject not in SUBJECT_VCFS:
		SUBJECT_VCFS[subject] = local
	germline = [w.replace('snpEff','annotated') for w in local]
	add_to_SUBJECT_ANNO(subject,"germline",germline)	

for sample in config['sample_references'].keys():
	local  = []
	subject=SAMPLE_TO_SUBJECT[sample]
	local.extend(
		[ (subject+"/"+sample+"/calls/"+sample+".MuTect.snpEff.txt"),
		(subject+"/"+sample+"/calls/"+sample+".strelka.snvs.snpEff.txt"),
		(subject+"/"+sample+"/calls/"+sample+".strelka.indels.snpEff.txt")
		]
	)	
	COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".copyNumber.new.txt"]
	COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".hq.new.txt"]
	COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.new.annotated.txt"]
	COPY_NUMBER +=[subject+"/"+sample+"/copyNumber/"+sample+".CN.new.filtered.txt"]
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".MuTect.annotated.txt"]
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.snvs.annotated.txt"]
	SOMATIC +=[subject+"/"+sample+"/calls/"+sample+".strelka.indels.annotated.txt"]
	if subject in SUBJECT_VCFS:
		SUBJECT_VCFS[subject].extend(local)

	somatic = [w.replace('snpEff','annotated') for w in local]
	if sample in config['sample_RNASeq']:
		somatic = [w.replace('MuTect.annotated','MuTect.annotated.expressed') for w in somatic]
		somatic = [w.replace('strelka.snvs.annotated','strelka.snvs.annotated.expressed') for w in somatic]
	add_to_SUBJECT_ANNO(subject,"somatic",somatic)
###########################################################################
###########################################################################
ALL_EXPRESSED =[]
expressedPairs = {}
if len(config['sample_RNASeq']) > 0:
	for Tumor in config['sample_RNASeq']:
		for RNASample in config['sample_RNASeq'][Tumor]:
			subject=SAMPLE_TO_SUBJECT[Tumor]
			RNASeqBam    = subject + "/" + RNASample + "/"+RNASample+".star.final.bam"
			expressedPairs[Tumor] = RNASeqBam
			ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.MuTect.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor],  sample=Tumor)]
			ALL_EXPRESSED += ["{subject}/{sample}/calls/{sample}.strelka.snvs.annotated.expressed.txt".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor)]
			
###########################################################################
# we have to do it this way as some samples may not have rna or tumor     #
###########################################################################
varFiles=[]
DBFiles =[]
ActionableFiles =[]
for subject in SUBJECT_ANNO.keys():
	for group in SUBJECT_ANNO[subject].keys():
		DBFiles +=[subject+"/"+subject+"/db/"+subject+"."+group] 
		ActionableFiles +=[subject+ACT_DIR+subject+"."+group+".actionable.txt"] 
		for varFile in SUBJECT_ANNO[subject][group]:
			varFiles.append(varFile)
###########################################################################
localrules: Khanlab_Pipeline, RNASeq, IGV_Session, DBinput,AttachAnnotation,Expressed,vcf2txt,symlink_tophatBam 
###########################################################################
#                               RNASeq Rules                              #
###########################################################################
include: NGS_PIPELINE +"/rnaseq_pipeline.rules"
include: NGS_PIPELINE +"/scripts/igv_snapshot.rules"
ALL_VCFs =[]
for subject in SUBJECT_VCFS.keys():
	for vcf in SUBJECT_VCFS[subject]:
		vcf = vcf.replace('snpEff.txt', 'raw.vcf')
		ALL_VCFs +=[vcf]
		vcf = vcf.replace('raw.vcf', 'raw.snpEff.vcf')
		ALL_VCFs +=[vcf]
rule Khanlab_Pipeline:
	input: 
		SUB_IGV.values(),
		COPY_NUMBER,
		ALL_VCFs,
		expand("{subject}/annotation/{subject}.Annotations.coding.rare.txt", subject=SUBS),
		expand("{subject}/qc/{subject}.coveragePlot.png", subject=SUBS),
		expand("{subject}/qc/{subject}.circos.png", subject=SUBS),
		expand("{subject}/qc/{subject}.hotspot_coverage.png", subject=SUBS),
		expand("{subject}/igv/session_{subject}.xml", subject=SUBS),
		"rnaseqDone",
		"QC_AllSamples.txt",
		ALL_QC, ALL_FASTQC,
		varFiles,
		DBFiles,
		ActionableFiles
	version: "1.0"
	params: 
		group =config["group"],
		wait4job= NGS_PIPELINE + "/scripts/" + config["block"]
	shell: """
	#######################
#	find log/ -type f -empty -delete
#	find . -group $USER -exec chgrp {params.group} {{}} \;
#	find . \( -type f -user $USER -exec chmod g+r {{}} \; \) , \( -type d -user $USER -exec chmod g+rwxs {{}} \; \)
#	cd /data/khanlab/projects/Genotyping/
#	Final=`sh genotype.sh`
#	perl {params.wait4job} ${{Final}}
#	echo -e "Pipeline finished successfully.\\n\\n\\n\\nRegards,\\nKhanLab\\nOncogenomics Section\\nCCR NCI NIH" |mutt -s "Khanlab NGS Pipeline" `whoami`@mail.nih.gov
	#######################
	"""
############
#	FASTQC
############
rule fastqc:
	input:
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz",
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz"
	output: 
		"{base}/{sample}/qc/fastqc/{sample}_R1_fastqc.html", 
		"{base}/{sample}/qc/fastqc/{sample}_R2_fastqc.html"
	log: "log/fastqc.{sample}"
	version: config["fastqc"]
	params:
		rulename  = "fastqc",
		batch     = config["job_fastqc"]
	shell: """
	#######################
	module load fastqc/{version}
	fastqc --extract -t 6 -o {wildcards.base}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R1]} 2>> {log}
	fastqc --extract -t 6 -o {wildcards.base}/{wildcards.sample}/qc/fastqc/ -d /scratch {input[R2]} 2>> {log}
	#######################
	"""
############
#       BWA
############
rule BWA:
	input: 
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz", 
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz",
		ref=config["bwaIndex"]
	output: 
		temp("{base}/{sample}/{sample}.bwa.bam"),
		temp("{base}/{sample}/{sample}.bwa.bam.bai")
	log: "log/bwa.{sample}"
	version: config["bwa"]
	params:
		rulename  = "bwa",
		platform  = config["platform"],
		samtools  = config["samtools"], 
		batch     = config["job_bwa"]
	shell: """
	#######################
	module load bwa/{version}
	module load samtools/{params.samtools}
	bwa mem -M \
	-t ${{SLURM_CPUS_ON_NODE}}\
	-R '@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tLB:{wildcards.sample}\tPL:{params.platform}' \
	{input.ref} {input.R1} {input.R2} 2> {log} | samtools view -Sbh - \
	| samtools sort -m 30000000000 - {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa
	samtools index {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa.bam
	#######################
	"""
############
#	Novoalign
############
rule Novoalign:
	input:
		R1=DATA_DIR + "/{sample}/{sample}_R1.fastq.gz",
		R2=DATA_DIR + "/{sample}/{sample}_R2.fastq.gz",
		index=config["Novo_index"]
	output:
		temp("{subject}/{sample}/{sample}.novo.bam"),
		temp("{subject}/{sample}/{sample}.novo.bam.bai")
	log: "log/novoalign.{sample}"
	version: config["novocraft"]
	params:
		rulename  = "novoalign",
		batch     = config["job_novoalign"],
		samtools  = config["samtools"],
		platform  = config["platform"]
	shell: """
	#######################
	module load samtools/{params.samtools}
	module load novocraft/{version}
#	mpiexec  -envall -host `scontrol show hostname ${{SLURM_NODELIST}} | paste -d',' -s` -np ${{SLURM_NTASKS}} novoalignMPI -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -t 250 --hlimit 7 -p 5,2 -l 30 -e 100 -i 200 100 -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -H -d {input.index} -f {input.R1} {input.R2} 2>{log}| samtools view -uS - | samtools sort -m 30000000000 - {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo 
	novoalign -c ${{SLURM_CPUS_ON_NODE}} -F STDFQ -o SAM \"@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}\\tLB:{wildcards.sample}\\tPL:{params.platform}\" -t 250 --hlimit 7 -p 5,2 -l 30 -e 100 -i 200 100 -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -H -d {input.index} -f {input.R1} {input.R2} 2>{log}| samtools view -uS - | samtools sort -m 30000000000 - {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo
	samtools index {wildcards.subject}/{wildcards.sample}/{wildcards.sample}.novo.bam 
	#######################
	"""
############
#       GenotypeFile
############
# Using Older version of samtools for this purpose
rule Genotyping:
	input:
		bam="{base}/{sample}/{sample}.{aligner}.final.bam",
		interval=config["genotypeBed"],
		ref=config["reference"],
		vcf2genotype=NGS_PIPELINE + "/scripts/" + config["vcf2genotype"],
		vcf2loh=NGS_PIPELINE + "/scripts/" + config["vcf2loh"],
	output:
		vcf="{base}/{sample}/calls/{sample}.{aligner}.samtools.vcf",
		gt="{base}/{sample}/qc/{sample}.{aligner}.gt",
		loh="{base}/{sample}/qc/{sample}.{aligner}.loh"
	log: "log/genotyping.{sample}"
	version: config["samtools_old"]
	params:
		rulename  = "genotype",
		batch     = config["job_genotype"],
		dest	  = config["genotypeDest"]
	shell: """
	#######################
	module load samtools/{version}
	samtools mpileup -u -C50 -f {input.ref} -l {input.interval} {input.bam} | bcftools view -gc - >{output.vcf} 2>{log}
	perl {input.vcf2genotype} {output.vcf} >{output.gt} 2>>{log}
	cp -f {output.gt} {params.dest}/{wildcards.sample}.gt 2>>{log}
	perl {input.vcf2loh} {output.vcf} >{output.loh} 2>>{log}
	#######################
	"""
############
#       BamQC
############
rule BamQC:
	input:
		bam="{base}/{sample}/{sample}.bwa.final.bam",
		bai="{base}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['sample_captures'][wildcards.sample].replace('.bed', '.gff')
	output:
		"{base}/{sample}/qc/BamQC/qualimapReport.html"
	log: "log/bamqc.{sample}"
	version: config["qualimap"]
	params: 
		rulename = "bamqc",
		batch	 = config["job_qualimap"],
		outdir	 ="{base}/{sample}/qc/BamQC",
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load qualimap/{version}
	qualimap bamqc -c -bam {input.bam} -outdir {params.outdir} -gff {input.interval} -nt ${{SLURM_CPUS_ON_NODE}} --java-mem-size=${{MEM}}G > {log} 2>&1
	#######################
	"""
############
#       QC_Sum
############
rule QC_Sum:
	input:
		ALL_QC,
		ALL_FASTQC,
		convertor = NGS_PIPELINE + "/scripts/" + config["makeQC"]
	output:
		"QC_AllSamples.txt"
	log: "log/qc_sum"
	version: "v1.1"
	params:
		rulename = "qc_sum",
		batch    = config['job_default']
	shell: """
	#######################
	perl {input.convertor} `pwd` >{output} 2>{log}
	#######################
	"""
############
#       Samtools flagstat
############
rule flagstat:
	input:	"{base}/{sample}/{sample}.{aligner}.final.bam"
	output: "{base}/{sample}/qc/{sample}.{aligner}.flagstat.txt"
	log:    "log/flagstat.{sample}"
	version: config["samtools"]
	params:
		rulename  = "flagstat",
		batch     = config["job_flagstat"]
	shell: """
	#######################
	module load samtools/{version}
	samtools flagstat {input} > {output}
	#######################
	"""
############
#       Reads Count for every Bed Region on bam file
############
rule copyNumber:
	input:
		bam="{base}/{sample}/{sample}.bwa.final.bam",
		interval= lambda wildcards: config['sample_captures'][wildcards.sample],
		flagstat="{base}/{sample}/qc/{sample}.bwa.flagstat.txt",
		tool=NGS_PIPELINE+ "/scripts/" + config["copyNumber"]
	output:
		"{base}/{sample}/copyNumber/{sample}.count.tmp.txt"
	version: config["samtools"]
	params:
		rulename  = "CopyNumber",
		batch     = config["job_copynumber"]
	shell: """
	#######################
	module load samtools/{version}
	#TotalReads=`samtools view -bh -L {input.interval} {input.bam} |samtools flagstat - |head -1|sed -e 's/\s/\\t/g' |cut -f 1`
	TotalReads=`head -1 {input.flagstat} | sed -e 's/\s/\\t/g' |cut -f 1`
	split -d -l 12000 {input.interval} /lscratch/${{SLURM_JOBID}}/input
	for file in /lscratch/${{SLURM_JOBID}}/input*
	do
		sh {input.tool} ${{TotalReads}} ${{file}} {input.bam} ${{file}}.out &
	done
	wait;
	cat /lscratch/${{SLURM_JOBID}}/input*.out >{output}
	#######################
	"""
############
#       Somatic Copy Number
############
rule CN_LRR:
	input:
		files=lambda wildcards: somaticCopy[wildcards.Tumor],
		ref=config["gene_coord"],
		index=config["reference"].replace('.fasta', '.index.txt'),
		tool=NGS_PIPELINE+ "/scripts/" +config["AddGene"],
		cgc    = config["annovar_data"]+"geneLists/combinedList_030816",
		filter=NGS_PIPELINE+ "/scripts/filterCNV.pl"
	output:
		out="{subject}/{Tumor}/copyNumber/{Tumor}.copyNumber.new.txt",
		hq="{subject}/{Tumor}/copyNumber/{Tumor}.hq.new.txt",
		final="{subject}/{Tumor}/copyNumber/{Tumor}.CN.new.annotated.txt",
		filtered="{subject}/{Tumor}/copyNumber/{Tumor}.CN.new.filtered.txt"	
	params:
		rulename = "LRR",
		batch    = config["job_default"],
	shell: """
	#######################
	echo -e "#Chr\\tStart\\tEnd\\tNormalCoverage\\tTumorCoverage\\tRatio\\tLRR\\tGene(s)\\tStrand(s)" >{output.out}	
	paste {input.files} |cut -f 1-4,8 |awk '{{OFS="\\t"}};{{print $1,$2,$3,$4,$5,($5+1)/($4+1),log(($5+1)/($4+1))/log(2)}}' >{output.out}.temp1
	
	module load bedtools/2.25.0
	intersectBed -a {input.ref} -b {input.files[0]} >{output.out}.temp
	perl {input.tool} {output.out}.temp {output.out}.temp1 >>{output.out}

	awk '{{if($4>=30) print $0}}' {output.out} >{output.hq}
	mean=`cut -f7 {output.hq}|grep -v LRR |awk '{{sum+=$1}} END {{printf("%d\\n", sum/NR)}}'`
	stdDev=`cut -f7 {output.hq}|grep -v LRR |awk '{{sum+=$1; sumsq+=$1*$1}} END {{print sqrt(sumsq/NR - (sum/NR)**2)}}'`
	min=`echo "${{mean}}-(2*${{stdDev}})"|bc`
	max=`echo "${{mean}}+(2*${{stdDev}})"|bc`

	perl {input.filter} filter {output.out} ${{min}} ${{max}} {input.cgc} |sort |uniq |sortBed -faidx {input.index} -header -i - >{output.final}
	cp -f {output.final} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.copyNumber.new.txt
	geneList=`grep -P "Gain|Loss" {output.final} |cut -f 11 |sort |uniq |grep -v "^-$"`
	
	head -1 {output.final} >{output.filtered}
	for gene in ${{geneList}};
	do
		awk -v gene=${{gene}} '{{if($11 == gene) print $0}}' {output.final}
	done |sort |uniq |sortBed -faidx {input.index} -header -i - >>{output.filtered}
	cp -f {output.filtered} {wildcards.subject}{ACT_DIR}/{wildcards.Tumor}.CN.new.filtered.txt
	rm -rf {output.out}.temp1 {output.out}.temp
	#######################
	"""
############
#       Hotspot Coverage
############
rule HotSpotCoverage:
	input:
		bam="{base}/{sample}/{sample}.{aligner}.final.bam",
		interval=config["hotspot_intervals"]
	output: "{base}/{sample}/qc/{sample}.{aligner}.hotspot.depth"
	log: "log/hotspotCoverage.{sample}"
	version: config["bedtools"]
	params:
		rulename  = "HotSpotCov",
		batch     = config["job_hotspot"],
		samtools  = config["samtools"]
	shell: """
	#######################
	module load samtools/{params.samtools} 
	module load bedtools/{version}
	samtools view -hF 0x400 -q 30 {input.bam} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {input.interval} >{output}
	#######################
	"""
############
# Coverage 
############
rule Coverage:
	input:
		bam="{subject}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
		interval= lambda wildcards: config['sample_captures'][wildcards.sample]
	output:
		"{subject}/{sample}/qc/{sample}.bwa.coverage.txt"
	log: "log/coverage.{sample}"
	version: config["bedtools"]
	params:
		rulename = "coverage",
		batch    = config["job_bedtools"]
	shell: """
	#######################
	module load bedtools/{version}
	bedtools coverage -abam {input.bam} -b {input.interval} -hist |grep "^all" > {output}
	#######################
	"""
############
# CoveragePlot
############
rule CoveragePlot:
	input: 
		covFiles=lambda wildcards: SUB_COV[wildcards.subject],
		coverage =NGS_PIPELINE + "/scripts/" + config["coverage"]
	output: "{subject}/qc/{subject}.coveragePlot.png",
	log: "log/coverage.{subject}"
	version: config["R"]
	params:
		rulename = "covplot",
		batch    = config["job_covplot"]
	shell: """
	#######################
		
	cp -f {input.covFiles} {wildcards.subject}/qc/ 
		
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.coverage}
	#######################
	"""
############
# Circos Plot
############
rule Circos:
	input:
		lohFiles=lambda wildcards: SUB_LOH[wildcards.subject],
		circos =NGS_PIPELINE + "/scripts/" + config["circos"]
	output:
		"{subject}/qc/{subject}.circos.png",
	log: "log/circos.{subject}"
	version: config["R"]
	params:
		rulename = "Circos",
		batch    = config["job_covplot"]
	shell: """
	#######################
	cp -f {input.lohFiles} {wildcards.subject}/qc/
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.circos}
	#######################
	"""
############
# Box Plot Hotspot
############
rule BoxPlot_Hotspot:
	input:
		covFiles=lambda wildcards: SUB_HOT[wildcards.subject],
		boxplot =NGS_PIPELINE + "/scripts/" + config["boxplot"]
	output:
		"{subject}/qc/{subject}.hotspot_coverage.png",
	log: "log/boxplot.{subject}"
	version: config["R"]
	params:
		rulename = "Boxplot",
		batch    = config["job_covplot"]
	shell: """
	#######################
	cp -f {input.covFiles} {wildcards.subject}/qc/
	module load R/{version}
	R --vanilla --slave --silent --args {wildcards.subject}/qc/ {output} {wildcards.subject} <{input.boxplot}
	#######################
	"""
############
#       Picard Mark Duplicates
############
rule Picard_MarkDup:
	input:
		bam="{subject}/{sample}/{sample}.{base}.bam",
		bai="{subject}/{sample}/{sample}.{base}.bam.bai"
		
	output: 
		bam=temp("{subject}/{sample}/{sample}.{base}.dd.bam"),
		index=temp("{subject}/{sample}/{sample}.{base}.dd.bam.bai"),
		metrics="{subject}/{sample}/qc/{base}.markdup.txt"
	log:    "log/markdup.{sample}.{base}"
	version: config["picard"]
	params:
		rulename  = "mark_dup",
		batch     = config["job_markdup"],
		samtools  = config["samtools"]    
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load picard/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $PICARDJARPATH/MarkDuplicates.jar AS=true M={output.metrics} I={input.bam} O={output.bam} REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT > {log} 2>&1
	module load samtools/{params.samtools}
	samtools index {output.bam}
	######################
	"""
############
#       GATK Best Practices
############
rule GATK:
	input: 	bam="{base}/{sample}/{sample}.bwa.dd.bam",
		bai="{base}/{sample}/{sample}.bwa.dd.bam.bai",
		ref=config["reference"],
		phase1=config["1000G_phase1"],
		mills=config["Mills_and_1000G"]
	output:
		bam="{base}/{sample}/{sample}.bwa.final.bam",
		index="{base}/{sample}/{sample}.bwa.final.bam.bai",
	log:    "log/gatk.{sample}"
	version: config["GATK"]
	params:
		rulename  = "gatk",
		batch     = config["job_gatk"]
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load GATK/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T RealignerTargetCreator -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals > {log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T IndelRealigner -R {input.ref} -known {input.phase1} -known {input.mills} -I {input.bam} --targetIntervals /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.realignment.intervals -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam >>{log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T BaseRecalibrator -R {input.ref} -knownSites {input.phase1} -knownSites {input.mills} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt >>{log} 2>&1
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T PrintReads -R {input.ref} -I /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.lr.bam -o {output.bam} -BQSR /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.recalibration.matrix.txt >>{log} 2>&1
	mv -f {wildcards.base}/{wildcards.sample}/{wildcards.sample}.bwa.final.bai {output.index}
	######################
	"""
############
#	Bam2MPG
############
rule Bam2MPG:
	input:
		bam="{subject}/{sample}/{sample}.novo.dd.bam",
		bai="{subject}/{sample}/{sample}.novo.dd.bam.bai",
		ref=config["reference"],
		interval= lambda wildcards: config['sample_captures'][wildcards.sample],
	output:
		snps="{subject}/{sample}/calls/{sample}.bam2mpg.vcf.gz",
	log: "log/bam2mpg.{sample}"
	version: config["bam2mpg"]
	params:
		rulename  = "bam2mpg",
		batch     = config["job_bam2mpg"],
		samtools  = config["samtools"],
		vcftools  = config["vcftools"]
	shell: """
	#######################
	if [ -f {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz ]; then
        	rm -rf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.*
		
	fi

	module load bam2mpg/{version}
	module load vcftools/{params.vcftools}
	for CHR in `seq 1 22` X Y;
	do
	bam2mpg --qual_filter 20 -bam_filter '-q31' --region chr${{CHR}} --only_nonref --snv_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf --div_vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf {input.ref} {input.bam} &
	done
	wait

	for CHR in `seq 1 22` X Y
	do
		cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf
		mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
		bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf
		cat /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf | vcf-sort >/lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf
		mv -f /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
		bgzip /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf
		tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.snps.vcf.gz
		tabix -p vcf /lscratch/${{SLURM_JOBID}}/chr${{CHR}}{wildcards.sample}.indel.vcf.gz
	done
	echo "Combine chr level vcf files"
	vcf-concat /lscratch/${{SLURM_JOBID}}/chr*{wildcards.sample}.*.vcf.gz >/lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf
	echo "Restrict to Bed file"

	vcftools --vcf /lscratch/${{SLURM_JOBID}}/{wildcards.sample}.snps.vcf --bed {input.interval} --out {wildcards.sample} --recode --keep-INFO-all

	sed -e 's/SAMPLE/{wildcards.sample}/g' {wildcards.sample}.recode.vcf |vcf-sort >{wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf

	bgzip {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf
	tabix -f -p vcf {wildcards.subject}/{wildcards.sample}/calls/{wildcards.sample}.bam2mpg.vcf.gz
	rm -rf {wildcards.sample}.recode.vcf
	
	#######################
	"""
############
# Subject Bam2MPG
############
rule Sub_MPG:
	input:
		vcf=lambda wildcards: SUB_MPG[wildcards.subject],
#		convertor= NGS_PIPELINE + "/scripts/" + config["vcffilter"]
	output:
		vcf="{subject}/{subject}/calls/{subject}.bam2mpg.raw.vcf"
	log: "log/mpg.{subject}"
	version: config["vcftools"]
	params:
		rulename = "mergevcf",
		batch    = config["job_default"],
		vcforder = NGS_PIPELINE + "/scripts/" +config["vcfOrderCol"]
	shell: """
	#######################
	module load vcftools/{version}
	module load R
	vcf-merge {input.vcf} > {output.vcf}.tmp
	{params.vcforder} -i {output.vcf}.tmp -o {output.vcf}
	rm -rf {output.vcf}.tmp
	#######################
	"""
############
#       MuTect
############
rule MuTect:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		cosmic=config["cosmic"],
		interval=lambda wildcards: pairedCapture[wildcards.Tumor]
	output:
		vcf="{subject}/{Tumor}/calls/{Tumor}.MuTect.raw.vcf",
		call_stats="{subject}/{Tumor}/qc/{Tumor}.mutect.call_stats.txt",
		coverage="{subject}/{Tumor}/qc/{Tumor}.mutect.coverage.wig.txt"
	log:	"log/mutect.{Tumor}"
	version: config["MuTect"]
	params:
		rulename = "MuTect",
		batch    = config["job_mutect"],
		vcforder = NGS_PIPELINE + "/scripts/" +config["vcfOrderCol"],
		mt       = "--max_alt_allele_in_normal_fraction 0.05 --max_alt_alleles_in_normal_count 4"
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	gawk '{{print $1 "\t" $2-10 "\t" $3+10}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
	module load muTect/{version}
	module load R
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $MUTECTJARPATH -T MuTect \
		--reference_sequence {input.ref} \
		--cosmic {input.cosmic} \
		--dbsnp {input.dbsnp} \
		--input_file:normal {input[2]} \
		--input_file:tumor {input[0]} \
		--intervals  /lscratch/${{SLURM_JOBID}}/target_intervals.bed \
		--coverage_file {output.coverage} \
		--out {output.call_stats} \
		--vcf {output.vcf}.vcf \
		{params.mt}\
		> {log} 2>&1
	
	{params.vcforder} -i {output.vcf}.vcf -o {output.vcf}
	rm -rf {output.vcf}.vcf
	#######################
	"""
############
#       Strelka
############
rule Strelka:
	input:
		lambda wildcards: somaticPairs[wildcards.Tumor],
		ref=config["reference"],
		config=config["strelka_config"],
		interval=lambda wildcards: pairedCapture[wildcards.Tumor]
	output:
		snps="{subject}/{Tumor}/calls/{Tumor}.strelka.snvs.raw.vcf",
		indels="{subject}/{Tumor}/calls/{Tumor}.strelka.indels.raw.vcf"
	log:    "log/strelka.{Tumor}"
	version: config["strelka"]
	params:
		rulename = "Strelka",
		batch    = config["job_strelka"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	gawk '{{print $1 "\t" $2-10 "\t" $3+10}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
	module load strelka/{version}
	configureStrelkaWorkflow.pl --normal={input[2]} --tumor={input[0]}\
	--ref={input.ref} --config={input.config} --output-dir=/lscratch/${{SLURM_JOBID}}/strelka > {log} 2>&1
	make -j ${{SLURM_CPUS_PER_TASK}} -f /lscratch/${{SLURM_JOBID}}/strelka/Makefile 2>> {log}
	module load vcftools/{params.vcftools}
	vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.snvs.vcf --bed /lscratch/${{SLURM_JOBID}}/target_intervals.bed --out {output.snps} --recode --keep-INFO-all
	mv -f {output.snps}.recode.vcf {output.snps}
	vcftools --vcf /lscratch/${{SLURM_JOBID}}/strelka/results/passed.somatic.indels.vcf --bed /lscratch/${{SLURM_JOBID}}/target_intervals.bed --out {output.indels} --recode --keep-INFO-all
	mv -f {output.indels}.recode.vcf {output.indels}
	NORMAL=`basename {input[2]} .bwa.final.bam`
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.snps}
	sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t{wildcards.Tumor}/g" {output.indels}
	
	#######################
	"""
############
# Subject Hapcaller
############
rule Sub_HapCall:
	input:
		bams=lambda wildcards: SUB_BAMS[wildcards.subject],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		interval= lambda wildcards: config['sample_captures'][wildcards.subject]
	output:
		vcf="{subject}/{subject}/calls/{subject}.hapcaller.raw.vcf"
	log: "log/hapcaller.{subject}"
	version: config["GATK"]
	params:
		rulename = "HC",
		batch    = config["job_gatk"]
	run:
		bamFiles = " ".join('-I ' + bam for bam in input.bams)
		shell("""
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load GATK/{version}
	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {input.interval} > /lscratch/${{SLURM_JOBID}}/target_intervals.bed
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar $GATK_JAR -T HaplotypeCaller -R {input.ref} {bamFiles} -L /lscratch/${{SLURM_JOBID}}/target_intervals.bed -o {output.vcf} --dbsnp {input.dbsnp} -mbq 20 -mmq 30 -log {log}
	#######################
	""")
############
# Subject Platypus
############
rule Sub_Platypus:
	input:
		bams=lambda wildcards: SUB_BAMS[wildcards.subject],
		ref=config["reference"],
		dbsnp=config["dbsnp"],
		interval= lambda wildcards: config['sample_captures'][wildcards.subject]
	output:
		vcf="{subject}/{subject}/calls/{subject}.platypus.raw.vcf"
	log: "log/platypus.{subject}"
	version: config["platypus"]
	params:
		rulename = "PLAT",
		batch    = config["job_platypus"]
	shell: """
	#######################
	module load platypus/{version}
	LIST=`echo {input.bams}|sed -e 's/ /,/g'`
	platypus callVariants --nCPU=${{SLURM_CPUS_ON_NODE}} --bufferSize=1000000 --maxReads=100000000 --bamFiles=${{LIST}} --regions={input.interval} --output={output.vcf} --refFile={input.ref} --logFileName={log}
	sed -i 's/.bwa.final//g' {output.vcf}
	#######################
	"""
############
#       FreeBayes 	** Not in use **
############
rule  FreeBayes:
	input:
		bam="{subject}/{sample}/{sample}.bwa.final.bam",
		bai="{subject}/{sample}/{sample}.bwa.final.bam.bai",
		ref=config["reference"],
		interval= lambda wildcards: config['sample_captures'][wildcards.sample]
	output:
		vcf="{subject}/{sample}/calls/{sample}.freebayes.vcf"
	log:    "log/freebayes.{sample}"
	version: config["freebayes"]
	params:
		rulename = "freebayes",
		batch    = config["job_freebayes"],
		vcftools = config["vcftools"]
	shell: """
	#######################
	module load freebayes/{version}
	freebayes -f {input.ref} --haplotype-length 50 -b {input.bam} -v {output.vcf} > {log} 2>&1
	module load vcftools/{params.vcftools}
	vcftools --vcf {output} --bed {input.interval} --out {output.vcf} --recode --keep-INFO-all
	mv -f {output.vcf}.recode.vcf {output.vcf}
	#######################
	"""
############
#	snpEff
############
rule snpEff:
	input:
		vcf="{subject}/calls/{base}.raw.vcf",
		ref=config["reference"],
		snpEff_config=config["snpEff_config"],
	output:
		eff="{subject}/calls/{base}.raw.snpEff.vcf"
	log: "log/snpEff.{base}"
	version: config["snpEff"]
	params:
		rulename      ="snpEff",
		batch	      =config["job_snpeff"],
		snpEff_genome =config["snpEff_genome"],
		annovar       =config["annovar"]
	shell: """
	#######################
	MEM=`echo "{params.batch}" |perl -n -e'/--mem=(\d+)/ && print \$1'`
	module load snpEff/{version}
	java -Xmx${{MEM}}g -Djava.io.tmpdir=/lscratch/${{SLURM_JOBID}} -jar ${{SNPEFFHOME}}/SnpSift.jar dbnsfp -c {input.snpEff_config} -a {input.vcf} | java -Xmx${{MEM}}g -jar ${{SNPEFFHOME}}/snpEff.jar -t -canon {params.snpEff_genome} > {output.eff} 2> {log}
	#######################
	"""
############
#       vcf2txt
############
rule vcf2txt:
	input:
		eff="{subject}/calls/{base}.raw.snpEff.vcf",
		vcf2txt=NGS_PIPELINE + "/scripts/" + config["vcf2txt"]
	output:
		txt="{subject}/calls/{base}.snpEff.txt"
	params:
		rulename      ="snpEff",
		batch         =config["job_default"],
		annovar       =config["annovar"]
	shell: """
	#######################
	module load annovar/{params.annovar}
	perl {input.vcf2txt} {input.eff} /spin1/scratch/ >{output.txt}
	#######################
	"""

############
#	MakeList
############
rule FormatInput:
	input:
		txtFiles=lambda wildcards: SUBJECT_VCFS[wildcards.subject],
		convertor= NGS_PIPELINE + "/scripts/" + config["annoInput"]
	output: 
		temp("{subject}/annotation/AnnotationInput.anno"), 
		temp("{subject}/annotation/AnnotationInput.sift")
	log: "log/FormatInput.{subject}"
	version: config["annovar"]
	params:
		rulename   = "FormatInput",
		batch      = config["job_default"],
		fAEV       = NGS_PIPELINE + "/scripts/" + config["fAEV"]
	shell: """
	#######################
	module load annovar/{version}
	cut -f 1-5 {input.txtFiles} |sort |uniq > {wildcards.subject}/annotation/allSites
	perl {params.fAEV} {wildcards.subject}/annotation/allSites annovar/AnnotationInput.final.txt {wildcards.subject}/annotation/AnnotationInput.annotations.final.txt {wildcards.subject}/annotation/AnnotationInput
	perl {input.convertor} {wildcards.subject}/annotation/AnnotationInput 2>{log}
	rm -rf "{wildcards.subject}/annotation/AnnotationInput.pph",
	#######################
	"""
############
#	Custom Annotation
############
rule Annotation:
	input: 
		"{subject}/annotation/AnnotationInput.anno",
		TableAnnovar=NGS_PIPELINE + "/scripts/" + config["Annovar"],
		custom     =NGS_PIPELINE + "/scripts/" + config["customAnno"]
	output:
		temp("{subject}/annotation/AnnotationInput.docm")
	log: "log/Annovar.{subject}"
	version: config["annovar"]
	params:
		rulename   = "Annotation",
		batch      = config["job_annovar"],
		RefData    = config["annovar_data"],
		build      = config["build"],
	shell: """
	#######################
	module load annovar/{version}
	sh {input.TableAnnovar} {wildcards.subject}/annotation AnnotationInput {input.custom} {params.RefData} 2>>{log}
	#######################
	"""
############
#       SIFT
############
rule SIFT:
	input:
		sift="{subject}/annotation/AnnotationInput.sift",
		convertor  = NGS_PIPELINE + "/scripts/" + config["SiftParse"]
	output: 
		temp("{subject}/annotation/AnnotationInput.sift.out")
	log: "log/SIFT.{subject}"
	version: config["SIFT"]
	params:
		rulename   = "SIFT",
		batch      = config["job_SIFT"],
		build      = config["SIFTbuild"]
	shell: """
	#######################
	if [ -s {input.sift} ]; then
		module load python/2.7.4
		module load SIFT/{version}
		DIR=`pwd`
		cd ${{DIR}}/`dirname {input.sift}`
		FILE=`basename {input.sift}` 
		SIFT_exome_nssnvs.pl -i ${{FILE}} -d $SIFTDB/Human_db_37 -o $SIFT_SCRATCHDIR -z ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv
		perl {input.convertor} ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv >${{DIR}}/`dirname {input.sift}`/${{FILE}}.out
	else
		echo -e "Chr\\tStart\\tEnd\\tRef\\tAlt\\tSIFT Prediction\\tSIFT Score" >{output}
	fi
	#######################
	"""
############
#       PPH2   ** Not in use**
############
rule PPH2:
	input:
		pph="{subject}/annotation/AnnotationInput.pph",
		convertor  = NGS_PIPELINE + "/scripts/" + config["PPH2Parse"],
		block      = NGS_PIPELINE + "/scripts/" + config["block"]
	output: "{subject}/annotation/AnnotationInput.pph2.out"
	log: "log/PPH2.{subject}"
	version: config["polyphen2"]
	params:
		rulename   = "PPH2",
		batch      = config["job_PPH2"],
	shell: """
	#######################
	module load polyphen2/{version}
	if [ -s {input.pph} ]; then
		mapsnps.pl -c -g hg19 -U -y {input.pph}.intermediate {input.pph} 2>{log}
		pph_swarm.pl {input.pph}.intermediate -d /scratch/`whoami`/${{RANDOM}}${{RANDOM}} -o {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt --partition ${{SLURM_JOB_PARTITION}} --block
		perl {input.convertor}  {wildcards.subject}/annotation/AnnotationInput.pph2.intermediate.txt >{output} 2>>{log}
	else 
		touch {output}
	fi
	rm -rf {wildcards.subject}/annotation/AnnotationInput.pph.inter*
	#######################
	"""
############
#	Combine Annotation
############
rule CombineAnnotation:
	input:
		anno="{subject}/annotation/AnnotationInput.docm", 
		sift="{subject}/annotation/AnnotationInput.sift.out", 
		convertor  = NGS_PIPELINE + "/scripts/" + config["CombineAnno"],
		geneanno   = NGS_PIPELINE + "/scripts/" + config["GeneAnno"],
		filter     = NGS_PIPELINE + "/scripts/" + config["filterVars"],
		coding     = NGS_PIPELINE + "/scripts/" + config["proteincoding"],
		blacklisted      = config["annovar_data"]+ "hg19_blacklistedSites.txt"
	output: "{subject}/annotation/{subject}.Annotations.coding.rare.txt"
	version: "1.0"
	params:
		rulename   = "combine",
		batch	   = config["job_Combine"],
		dataDir    = config["annovar_data"]
	shell: """
	#######################
	echo "{wildcards.subject}/annotation/AnnotationInput
{wildcards.subject}/annotation/AnnotationInput.anno.gene
{wildcards.subject}/annotation/AnnotationInput.anno.exac.3
{wildcards.subject}/annotation/AnnotationInput.anno.clinseq
{wildcards.subject}/annotation/AnnotationInput.anno.cadd
{wildcards.subject}/annotation/AnnotationInput.sift.out
{wildcards.subject}/annotation/AnnotationInput.clinvar
{wildcards.subject}/annotation/AnnotationInput.anno.cosmic
{wildcards.subject}/annotation/AnnotationInput.hgmd
{wildcards.subject}/annotation/AnnotationInput.match
{wildcards.subject}/annotation/AnnotationInput.docm
{wildcards.subject}/annotation/AnnotationInput.candl
{wildcards.subject}/annotation/AnnotationInput.tcc
{wildcards.subject}/annotation/AnnotationInput.mcg
{wildcards.subject}/annotation/AnnotationInput.civic
{wildcards.subject}/annotation/AnnotationInput.anno.pcg" >{wildcards.subject}/annotation/list
	perl {input.convertor} {wildcards.subject}/annotation/list >{output}
	perl {input.geneanno} {params.dataDir}/hg19_ACMG.txt {output} >>{wildcards.subject}/annotation/AnnotationInput.annotations.final.txt
	perl {input.coding} {wildcards.subject}/annotation/AnnotationInput.annotations.final.txt | perl {input.filter} - {input.blacklisted} 0.1 |sort -n |uniq >{output}

	rm -rf {wildcards.subject}/annotation/AnnotationInput.pph {wildcards.subject}/annotation/AnnotationInput.anno.* {wildcards.subject}/annotation/AnnotationInput.hgmd {wildcards.subject}/annotation/AnnotationInput.match {wildcards.subject}/annotation/AnnotationInput.candl {wildcards.subject}/annotation/AnnotationInput.tcc {wildcards.subject}/annotation/AnnotationInput.mcg {wildcards.subject}/annotation/AnnotationInput.civic {wildcards.subject}/annotation/AnnotationInput.anno.pcg {wildcards.subject}/annotation/AnnotationInput.clinvar {wildcards.subject}/annotation/AnnotationInput {wildcards.subject}/annotation/allSites  
	#######################
	"""
############
#	Add Annotation back to sample level file 
############
rule AttachAnnotation:
	input:
		txt="{subject}/{base1}/calls/{base}.snpEff.txt",
		ref="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
		convertor  = NGS_PIPELINE + "/scripts/" + config["addBack"]
	output:
		txt="{subject}/{base1}/calls/{base}.annotated.txt"
	log: "log/attach_annotation.{subject}.{base}"
	version: "1.0"
	params:
		rulename   = "add",
		batch      = config["job_addbackann"],
	shell: """
	#######################
	perl {input.convertor} {input.ref}  {input.txt} >{output.txt} 2>{log}
	#######################
	"""
############
#       Expressed
############
rule Expressed:
	input: 
		RNASeq = lambda wildcards: expressedPairs[wildcards.sample],
		Mutation="{subject}/{sample}/calls/{base}.annotated.txt",
		convertor = NGS_PIPELINE + "/scripts/" + config["mpileup"]
	output: "{subject}/{sample}/calls/{base}.annotated.expressed.txt"
	log: "log/expressed.{base}"
	version: config["samtools"]
	params:
		rulename  = "Expressed",
		batch     = config["job_expressed"],
		convertor = config["mpileup"]
	shell: """
	#######################
	module load samtools/{version}
	perl {input.convertor} {input.Mutation} {input.RNASeq} > {output} 2>{log}
	#######################
	"""
############
#       Database Input
############
rule DBinput:
	input:
		txtFiles=lambda wildcards: SUBJECT_ANNO[wildcards.subject][wildcards.group],
		convertor=NGS_PIPELINE + "/scripts/" + config["DBinput"]
	output: "{subject}/{subject}/db/{subject}.{group}"
	params:
		rulename  = "makeDBinput"
	shell: """
	#######################
	perl {input.convertor} {input.txtFiles} >{output}
	#######################
	"""	
############
#       Actionable
############
rule Actionable_Somatic:
	input:
		somatic="{subject}/{subject}/db/{subject}.somatic",
		convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
		annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
		refFile= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
		cgc    = config["annovar_data"]+"geneLists/CancerGeneCensus.v76.txt",
		annotate  = NGS_PIPELINE + "/scripts/" + config["addBack"],
	output:
		somatic="{subject}/{ACT_DIR}/{subject}.somatic.actionable.txt",
	params:
		rulename  = "ActionableMutations",
		batch    = config['job_default']
	shell: """
	#######################
	perl {input.convertor} somatic  {input.refFile} {input.cgc} {input.somatic} {input.annotation} >{output.somatic}
	#######################
	"""
############
#       Actionable
############
rule Actionable_Germline:
	input:
		germline="{subject}/{subject}/db/{subject}.germline",
		convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
		annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
		annotate  = NGS_PIPELINE + "/scripts/" + config["addBack"],
		cancerGeneCensus = config["annovar_data"]+"geneLists/CGCensus_Hereditary.txt",
		hotspot= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
		tsid   = config["annovar_data"]+"geneLists/TruSightInheritedDiseases.txt",
		jsw    = config["annovar_data"]+"geneLists/CancerGenes_JSW.txt",
		clt2   = config["annovar_data"]+"geneLists/ClinomicsTier2GeneList.txt",
		ghr    = config["annovar_data"]+"geneLists/Genetics_HumanRef.3.8.16.txt"

	output:
		germline="{subject}/{ACT_DIR}/{subject}.germline.actionable.txt",
	params:
		rulename  = "ActionableMutations",
		batch    = config['job_default']
	shell: """
	#######################
	if [ -e {wildcards.subject}/{wildcards.subject}/db/{wildcards.subject}.somatic ]
	then 
		perl {input.convertor} germline {wildcards.subject}/{wildcards.subject}/db/{wildcards.subject}.somatic {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}
	else
		touch /spin1/scratch/temp
		perl {input.convertor} germline /spin1/scratch/temp                                                    {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}
	fi
	#######################
	"""
############
#       Actionable
############
rule Actionable_RNAseq:
	input:
		rnaseq="{subject}/{subject}/db/{subject}.rnaseq",
		convertor=NGS_PIPELINE + "/scripts/" + config["Actionable_mutation"],
		annotation="{subject}/annotation/{subject}.Annotations.coding.rare.txt",
		annotate  = NGS_PIPELINE + "/scripts/" + config["addBack"],
		cancerGeneCensus = config["annovar_data"]+"geneLists/CGCensus_Hereditary.txt",
		hotspot= config["annovar_data"]+"hg19_SomaticActionableSites.txt",
		tsid   = config["annovar_data"]+"geneLists/TruSightInheritedDiseases.txt",
		jsw    = config["annovar_data"]+"geneLists/CancerGenes_JSW.txt",
		clt2   = config["annovar_data"]+"geneLists/ClinomicsTier2GeneList.txt",
		ghr    = config["annovar_data"]+"geneLists/Genetics_HumanRef.3.8.16.txt"
	output:
		rnaseq="{subject}/{ACT_DIR}/{subject}.rnaseq.actionable.txt",
	params:
		rulename  = "ActionableMutations",
		batch    = config['job_default']
	shell: """
	#######################
	touch /spin1/scratch/temp
	perl {input.convertor} rnaseq /spin1/scratch/temp {input.rnaseq} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.rnaseq}
	#######################
	"""
############
#	**END**
############
